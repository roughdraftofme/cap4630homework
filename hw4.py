# -*- coding: utf-8 -*-
"""hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lWeTCxmLSurEFeIB6dVa4KbTeLCXCIJD

I was very excited to take AI this semester. I plan on focusing on AI and ML studies when I pursue my Masters degree starting next Fall, so I was eager to break into this area of study. I feel like I gained a very good idea of many different topics over the course of this semester.

# General Concepts

The first thing I learned this semester was python, which is an incredibly useful tool. I was thankful for the time at the start of the semester that I had to develop my skills and familiarity with a new language before I needed to utilize it in an assignment. This time to work independently was invaluable.

I also became very familiar with a lot of key terms in the Artifical Intelligence arena, and while I am certainly not an expert on any of it yet, I do feel comfortable enough associating certain terminologies with their correct association, such as activation functions and optimizers.

Artificial Intelligence is the canopy under which all of our studies this semester fall. AI is, essentially, any time a computer program does something smart. This is typically associated with the idea of a computer program doing something "on its own", or at least the appearance that the program is operating independently of human control.

In reality, this perceived independence is most commonly a result of careful programming techniques that merely imitate independent thought. Actual AI is not commercially accessible, and programs that are allowed to write their own code or continue to develop independently after their initialization are still only in a researching phase.

Machine Learning programs are a form of Artificial Intelligence. ML programs will change in response to data they are given, basically mimicking the idea of "learning" that we previously reserved as something only sentient beings were capable of.

The core use of ML programs is to train the program on some initial set of data and let it form the "rules" for determining the input and output mappings of that data. Then, we would expect that we can feed new data to the program that it has not yet seen and it would be able to accurately assign an output to that data. The most common application of ML models is image classification.

# Building A Model

The thing I was most interested in was learning how to put together a model. Dr. Wocjan put it very well when he said that this is "more of an art than a science." I learned this first hand when I built a Machine Learning model for a group project in my Processes of Object Oriented Software Development class. I designed a model that recognized 30 different animals based on photo input.

Building this model was a very harrowing experience. It took me many hours of work to build this model from scratch and continuously refine it after checking the training results. I adjusted the number of layers, the contents of each layer, the number of nodes in each component of the layers, and so on. I tracked the changes of these models over the course of every change to keep track of what worked better and what didn't.
"""

def create_model():
    model = models.Sequential()

    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=train_images.shape[1:]))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.4))
    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Dropout(0.5))
    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(30, activation='softmax'))

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

"""The model was incredibly overfit, and I couldn't get the validation accuracy to come down before the deadline for the project, but I am going to continue working on it over the break. I plan on implementing data augmentation in order to fight the overfitting, and continuing to make improvements to the layers of my model from there.

As you can see in the code, I stuck with relu, which is the Rectified Linear Unit. Relu is great for this model because it operates on tensors, and this model is for image classification. On the last layer where we split to the 30 expected categories, we swap to a softmax activation function, which will set the data between zero and one to how strongly the model believes the input matches each of the categories. This is a great activation function for the final layer of a model like this where we are expecting only one answer of what the model thinks the image is of. This is also why I chose to use categorical cross entropy as the loss function.

# Comping A Model

From the code above, you can see that I used adam as an optimizer. I tried out quite a few others, but adam still yielded the best results, even though the results are still not ideal. Stochastic gradient descent was another optimizer that I spent a lot of time testing out for use with my model. However, as soon as I got a model with some decent numbers, when I tried adam as the optimizer instead the results were significantly better. I, perhaps naively, stuck with adam after that and didn't continue trying out different optimizers. As I work more on developing this model, I will keep that in mind.

Learning rates are a vitally important part of an model. If the learning rate is not aggressive enough, it will allow the model to get stuck in a local minimum, leading to inaccurate results. Conversely, if the learning rate is too aggressive, it will never settle in a local minimum and results will also be inaccurate. Like all aspects of constructing a model, finding the right learning rate is a crucial aspect.

# Training A Model

Oddly enough, underfitting was never a problem when I was training my model for the project. I continuously ran into one of two situations:



1.   The difference between training and validation accuracy was extremely minimal, but the actual accuracy numbers were abysmal.
2.   The model was so overfit it wasn't even funny.

In the first case, I produced so many models where the loss and accuracy graphs for the model were so close to each other that it was amazing. An untrained observer might look at the comparisons of the red and blue plots on the graph and be highly impressed with how close they were. However, in these instances the accuracy never broke 45%, with loss tottering around 2.00 as well.

Over the break, it is my plan to return to some of these models and work on trying to decrease their loss and improve their accuracy. I strayed far from these results because I was trying to get better accuracy first and then worry about how closely fit it would be. Looking back now, I really think it ought to be the other way around.

In the second case, I found out that I am *really good* at writing overfit models. As I was trying to just boost accuracy, I found that getting the training accuracy up was actually incredibly simple - but the difference between the training and validation was horrible. I would often write models that broke 98% for training accuracy over 100 epochs, but would plateau between epoch 10 and 20 on the validation accuracy, lingering at around 30%. Comparatively, the loss for training would hang around 25% or so, but for validation it wasn't unusual to hit 300% or more.

These models helped me learn that trying to fight overfitting just by adjusting the layers, their sizes, or other such factors of the model, is a lost cause. It takes a lot more than just changing these items to fix an overfit model. Dropout helped to a certain extent, but I still could not get a good model produced. As I previously mentioned, I do plan to implement data augmentation for this model and see if that can help with the overfitting. I feel that, combined with working on improving accuracy from well-fit models, will set me on the right track to building a solid model for image classification.

# My Model

If you are interested and want to check out the entirety of my (extremely poor) model, here is a link to the notebook where I have been constructing it. As I am writing this, it is on an extremely overfit iteration of the model. Hopefully soon I will be able to get it to a better setting and be able to use it for a second round deployment of our project.


[Narwhal Model](https://colab.research.google.com/drive/1eYU-0AjiPw6EgloWvSDlfA2JWHrW7Isj)
"""